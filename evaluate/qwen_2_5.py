import h5py
import numpy as np
import cv2
import os
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

import re

def extract_pixel_coords(output_text):
    """
    ä»å¤§æ¨¡å‹è¾“å‡ºä¸­æå– (x, y) åƒç´ åæ ‡
    è‡ªåŠ¨å¤„ç† list / dict / str
    """

    # ---------- 1ï¸âƒ£ ç»Ÿä¸€æˆå­—ç¬¦ä¸² ----------
    if output_text is None:
        return []

    if isinstance(output_text, list):
        # å¸¸è§æƒ…å†µï¼š["text ..."]
        if len(output_text) == 0:
            return []
        output_text = output_text[0]

    if isinstance(output_text, dict):
        # å¸¸è§å­—æ®µå…œåº•
        for k in ["generated_text", "text", "content"]:
            if k in output_text:
                output_text = output_text[k]
                break

    if not isinstance(output_text, str):
        raise TypeError(f"extract_pixel_coords expects str, got {type(output_text)}")

    # ---------- 2ï¸âƒ£ å»æ‰ <think> ... </think> ----------
    text_wo_think = re.sub(
        r"<think>.*?</think>",
        "",
        output_text,
        flags=re.DOTALL
    )

    # ---------- 3ï¸âƒ£ æå– (x, y) ----------
    matches = re.findall(r"\(\s*(\d+)\s*,\s*(\d+)\s*\)", text_wo_think)

    coords = [(int(x), int(y)) for x, y in matches]

    return coords

def extract_bboxes(output_text):
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­æå– bounding boxes
    è¿”å›: List[(x1, y1, x2, y2)]
    """

    # -------- 1. ç»Ÿä¸€æˆå­—ç¬¦ä¸² --------
    if isinstance(output_text, list):
        output_text = output_text[0]
    if isinstance(output_text, dict):
        output_text = output_text.get("generated_text", "")

    if not isinstance(output_text, str):
        raise TypeError(f"expect str, got {type(output_text)}")

    # -------- 2. å»æ‰ <think> --------
    text = re.sub(r"<think>.*?</think>", "", output_text, flags=re.DOTALL)

    # -------- 3. åŒ¹é… (x1, y1, x2, y2) --------
    pattern = r"\(\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*,\s*(\d+)\s*\)"
    matches = re.findall(pattern, text)
    bboxes = [(int(x1), int(y1), int(x2), int(y2)) for x1, y1, x2, y2 in matches]

    return bboxes

import cv2
import os

def draw_points_on_image(
    img_path: str,
    points,
    save_path: str,
    radius: int = 6,
    color=(0, 0, 255),  # çº¢è‰² (BGR)
    thickness: int = -1  # å®å¿ƒåœ†
):
    """
    åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶åƒç´ ç‚¹å¹¶ä¿å­˜

    Args:
        img_path: åŸå§‹å›¾ç‰‡è·¯å¾„
        points: [(x, y), ...] åƒç´ åæ ‡
        save_path: ä¿å­˜çš„æ–°å›¾ç‰‡è·¯å¾„
        radius: ç‚¹çš„åŠå¾„
        color: BGR é¢œè‰²
        thickness: -1 è¡¨ç¤ºå®å¿ƒ
    """

    img = cv2.imread(img_path)
    if img is None:
        raise FileNotFoundError(f"Cannot load image: {img_path}")

    h, w = img.shape[:2]

    for i, (x, y) in enumerate(points):
        # è¾¹ç•Œä¿æŠ¤
        if 0 <= x < w and 0 <= y < h:
            cv2.circle(img, (x, y), radius, color, thickness)
            cv2.putText(
                img,
                str(i),
                (x + 5, y - 5),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.5,
                (255, 255, 255),
                1,
                cv2.LINE_AA
            )
        else:
            print(f"âš ï¸ point {i} out of bounds: {(x, y)}")

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    cv2.imwrite(save_path, img)
    print(f"âœ… Saved visualization to {save_path}")


def draw_bboxes_on_image(
    img_path,
    bboxes,
    save_path,
    color=(0, 255, 0),
    thickness=2
):
    """
    img_path: åŸå›¾è·¯å¾„
    bboxes: [(x1,y1,x2,y2), ...]
    save_path: è¾“å‡ºè·¯å¾„
    """

    img = cv2.imread(img_path)
    if img is None:
        raise FileNotFoundError(img_path)

    h, w = img.shape[:2]

    for (x1, y1, x2, y2) in bboxes:
        # é˜²æ­¢è¶Šç•Œ
        x1 = max(0, min(w - 1, x1))
        x2 = max(0, min(w - 1, x2))
        y1 = max(0, min(h - 1, y1))
        y2 = max(0, min(h - 1, y2))

        cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    cv2.imwrite(save_path, img)

    return img
# -------------------------------
# é…ç½®åŒº
# -------------------------------

# H5 æ–‡ä»¶è·¯å¾„
img_path = 'docs/2.jpg'

# åŠ è½½æ¨¡å‹
model_path = "checkpoints/MiMo-Embodied-7B"
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_path,
    torch_dtype="auto",
    device_map={"": 0},   # <<<<<<<< è¿™é‡Œï¼
)
processor = AutoProcessor.from_pretrained(model_path)



device = model.device  # è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡
print(f"âœ… æ¨¡å‹åŠ è½½åœ¨è®¾å¤‡ï¼š{device}")


instruction = "What do you see in these images?"
instruction = "ä½ çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿ æè¿°ä½ çœ‹åˆ°çš„ç‰©å“å¹¶è¾“å‡ºå›¾ç‰‡é‡Œå¯¹åº”ç‰©å“ä¸­å¿ƒç‚¹çš„åƒç´ åæ ‡ã€‚"
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": img_path},
            {"type": "text", "text": instruction},
        ],
    }
]

# -------------------------------
# å‡†å¤‡æ¨ç† inputs
# -------------------------------

# æ–‡æœ¬æ¨¡æ¿å¤„ç†
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)

# å›¾ç‰‡ã€è§†é¢‘é¢„å¤„ç†
image_inputs, video_inputs = process_vision_info(messages)


# æ•´åˆinputs
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)

# -------------------------------

for k, v in inputs.items():
    if isinstance(v, torch.Tensor):
        inputs[k] = v.to(device)

print("ğŸš€ å¼€å§‹æ¨ç†...")
generated_ids = model.generate(**inputs, max_new_tokens=4096)

# åªå–æ–°ç”Ÿæˆéƒ¨åˆ†
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]

# è§£ç è¾“å‡º
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
coords = extract_pixel_coords(output_text)
draw_points_on_image(
    img_path=img_path,
    points=coords,
    save_path="docs/pointed2-1.jpg"
)

bboxes = extract_bboxes(output_text)
draw_bboxes_on_image(
    img_path=img_path,
    bboxes=bboxes,
    save_path="docs/pointed2-2.jpg"
)


print("ğŸ“ æ¨ç†è¾“å‡ºï¼š")
# print(output_text)

for text in output_text:
    print(text)
